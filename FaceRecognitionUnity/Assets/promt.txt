Hi gpt lets assume you are the master of uml diagrams 
Here i have a project which works  like this which is given in next line 

first user go in unity application and write the name and then click on register button 
by clicking the name will be send to python for processing and python will start the capturing images and it will capture 30 frames then it will train the model using LBPHFaceRecognizer in open cv and it puts the model data in trainer.yml and save the user information and a id assigned to the face on firebase then user will click on login then it will again open an opencv window where the model will be used to recognize the face and the model will return the id if the id is there is in the firebase the the data will be retrieved from firebase and send it to unity using socket 

to accomlish this i made some files
first face login which is given below
import socket
from facerecognition import FaceRecognition
from facetraining import RegisterFaces
# create a UDP socket
sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)

# bind the socket to a specific IP address and port number
server_address = ('127.0.0.1', 8001)
sock.bind(server_address)
while True:
    # receive data from the socket
    try:
        data, address = sock.recvfrom(4096)
        option = data.decode('utf-8')
        if option[0] == "0":
            print("Login")   
            FaceRecognition()
        if option[0] == "1":
            print("SignUp")
            RegisterFaces(option[2:])
    except Exception as e:
        print(e)
        break


second file is face recognition which is given below


''''
Real Time Face Recogition
	==> Each face stored on dataset/ dir, should have a unique numeric integer ID as 1, 2, 3, etc                       
	==> LBPH computed model (trained faces) should be on trainer/ dir
Based on original code by Anirban Kar: https://github.com/thecodacus/Face-Recognition    

Developed by Marcelo Rovai - MJRoBot.org @ 21Feb18  

'''

import cv2
import numpy as np
import os
import socket
import zmq
# from facetraining import RegisterFaces
from database import GetValues, PushValues

font = cv2.FONT_HERSHEY_SIMPLEX

# iniciate id counter
id = 0

# names related to ids: example ==> Marcelo: id=1,  etc
names = GetValues()
sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)


def SendValue(data, port=8000):
    serverAddressPort = ("127.0.0.1", port)
    sock.sendto(str.encode(str(data)), serverAddressPort)


def FaceRecognition():
    recognizer = cv2.face.LBPHFaceRecognizer_create()
    recognizer.read('trainer/trainer.yml')
    cascadePath = "Cascades/haarcascade_frontalface_default.xml"
    faceCascade = cv2.CascadeClassifier(cascadePath)
    context = zmq.Context()
    socket = context.socket(zmq.PUSH)
    socket.bind('tcp://*:5555')

    # Initialize and start realtime video capture
    cam = cv2.VideoCapture(0)
    cam.set(3, 640)  # set video widht
    cam.set(4, 480)  # set video height

    # Define min window size to be recognized as a face
    minW = 0.1*cam.get(3)
    minH = 0.1*cam.get(4)
    count = 0
    while cam.isOpened():
        ret, img = cam.read()
        if not ret:
            continue

        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)

        faces = faceCascade.detectMultiScale(
            gray,
            scaleFactor=1.2,
            minNeighbors=5,
            minSize=(int(minW), int(minH)),
        )

        for (x, y, w, h) in faces:

            cv2.rectangle(img, (x, y), (x+w, y+h), (0, 255, 0), 2)

            id, confidence = recognizer.predict(gray[y:y+h, x:x+w])

            # Check if confidence is less them 100 ==> "0" is perfect match
            if (confidence < 100):
                id = names[id]
                SendValue(id)
            else:
                id = "unknown"

            cv2.putText(img, str(id), (x+5, y-5), font, 1, (255, 255, 255), 2)
        data = {
            'bool': True,
            'int': 123,
            'str': 'Hello there!',
            'image': cv2.imencode('.jpg', img)[1].ravel().tolist()
        }
        print(data)
        socket.send_json(data)

        k = cv2.waitKey(10) & 0xff  # Press 'ESC' for exiting video
        if k == 27:
            break
        if count > 100:
            break

    # Do a bit of cleanup
    print("\n [INFO] Exiting Program and cleanup stuff")
    cam.release()
    cv2.destroyAllWindows()


third class is face training which is given below 



''''
Training Multiple Faces stored on a DataBase:
	==> Each face should have a unique numeric integer ID as 1, 2, 3, etc                       
	==> LBPH computed model will be saved on trainer/ directory. (if it does not exist, pls create one)
	==> for using PIL, install pillow library with "pip install pillow"

Based on original code by Anirban Kar: https://github.com/thecodacus/Face-Recognition    

Developed by Marcelo Rovai - MJRoBot.org @ 21Feb18   

'''

import cv2
import numpy as np
from PIL import Image
import os
from database import GetValues, PushValues
import zmq


def RegisterFaces(name):
    context = zmq.Context()
    socket = context.socket(zmq.PUSH)
    socket.bind('tcp://*:5555')

    cam = cv2.VideoCapture(0)
    cam.set(3, 640)  # set video width
    cam.set(4, 480)  # set video height
    data = GetValues()
    dataLen = len(data)
    face_id = dataLen
    # For each person, enter one numeric face id
    faceName = name

    face_detector = cv2.CascadeClassifier(
        'Cascades/haarcascade_frontalface_default.xml')

    print("\n [INFO] Initializing face capture. Look the camera and wait ...")
    # Initialize individual sampling face count
    count = 0

    isRecording = False
    while cam.isOpened():

        ret, img = cam.read()
        if not ret:
            continue
        if isRecording:
            gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
            faces = face_detector.detectMultiScale(gray, 1.3, 5)

            for (x, y, w, h) in faces:

                cv2.rectangle(img, (x, y), (x+w, y+h), (255, 0, 0), 2)
                count += 1

                # Save the captured image into the datasets folder
                cv2.imwrite("dataset/User." + str(face_id) + '.' +
                            str(count) + ".jpg", gray[y:y+h, x:x+w])
        data = {
            'bool': True,
            'int': 123,
            'str': 'Hello there!',
            'image': cv2.imencode('.jpg', img)[1].ravel().tolist()
        }
        socket.send_json(data)
        cv2.imshow("Window",img)
        k = cv2.waitKey(100) & 0xff  # Press 'ESC' for exiting video
        if k == ord("r"):
            isRecording = True
        if k == 27:
            break
        elif count >= 30:  # Take 30 face sample and stop video
            break

    # Do a bit of cleanup
    print("\n [INFO] Exiting Program and cleanup stuff")
    cam.release()
    cv2.destroyAllWindows()
    if count >= 30:
        TrainDataSet(faceName)


def TrainDataSet(faceName):

    # Path for face image database
    path = 'dataset'

    recognizer = cv2.face.LBPHFaceRecognizer_create()
    detector = cv2.CascadeClassifier(
        "Cascades/haarcascade_frontalface_default.xml")

    # function to get the images and label data

    def getImagesAndLabels(path):

        imagePaths = [os.path.join(path, f) for f in os.listdir(path)]
        faceSamples = []
        ids = []

        for imagePath in imagePaths:

            PIL_img = Image.open(imagePath).convert(
                'L')  # convert it to grayscale
            img_numpy = np.array(PIL_img, 'uint8')

            id = int(os.path.split(imagePath)[-1].split(".")[1])
            faces = detector.detectMultiScale(img_numpy)

            for (x, y, w, h) in faces:
                faceSamples.append(img_numpy[y:y+h, x:x+w])
                ids.append(id)

        return faceSamples, ids

    print("\n [INFO] Training faces. It will take a few seconds. Wait ...")
    faces, ids = getImagesAndLabels(path)
    recognizer.train(faces, np.array(ids))

    # Save the model into trainer/trainer.yml
    # recognizer.save() worked on Mac, but not on Pi
    recognizer.write('trainer/trainer.yml')
    PushValues(faceName)
    # Print the numer of faces trained and end program
    print("\n [INFO] {0} faces trained. Exiting Program".format(
        len(np.unique(ids))))

so with this can you make class diagram
